{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributed\n",
    "\n",
    "println(nprocs())\n",
    "addprocs(4)         # add 4 workers\n",
    "println(nprocs())   # total number of processes\n",
    "println(nworkers()) # only worker processes\n",
    "rmprocs(workers())  # remove worker processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://groups.google.com/g/slurm-users/c/r_MNRw4gYhQ\n",
    "\n",
    "```\n",
    "Now, you can get fancy and have hybrid applications which can be split\n",
    "up across nodes (individual processes) but each one of those tasks can\n",
    "also use multiple cores at the same time by multi-threading.\n",
    "```\n",
    "\n",
    "So how would I split each parameter combination to different nodes and multithread across trials within each node?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://researchcomputing.princeton.edu/support/knowledge-base/scaling-analysis#hybrid\n",
    "\n",
    "```\n",
    "Hybrid Multithreaded, Multinode Codes\n",
    "Some codes take advantage of both shared- and distributed-memory parallelism (e.g., OpenMP and MPI). In these cases you will need to vary the number of nodes, ntasks-per-node and cpus-per-task. Construct a table as above except include a new column for cpus-per-task. Note that when taking full nodes, the product of ntasks-per-node and cpus-per-task should be equal to the total number of CPU-cores per node. Use the \"snodes\" command to find the total number of CPU-cores per node for a given cluster.\n",
    "\n",
    "Find the optimal values for these Slurm directives:\n",
    "\n",
    "#SBATCH --nodes=<M>\n",
    "#SBATCH --ntasks-per-node=<N>\n",
    "#SBATCH --cpus-per-task=<T>\n",
    "```\n",
    "\n",
    "In the Caltech cluster compute nodes have 32 cores/node. So not to waste resources I can request e.g.\n",
    "\n",
    "```\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks-per-node=8\n",
    "#SBATCH --cpus-per-task=4\n",
    "```\n",
    "where the NLL for 8 parameter combinations is computed concurrently with each computation using 4 threads to compute trial likelihoods in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `--cpus-per-task` and `--ntasks-per-node` options in batch scripts\n",
    "\n",
    "`--cpus-per-task` uses cpus on a single node. So it should not exceed the number of cores/node. `--ntasks-per-node` can be distributed across nodes. If it is >1 then `--cpus-per-task` * `--ntasks-per-node` should not exceed number of cores/node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So can I:  \n",
    "- Parallelize subjects across jobs using job arrays (`#SBATCH --array=0-4`)  \n",
    "- Parallelize models/parameter combinations across tasks using `--ntasks-per-node`  \n",
    "- Parallelize trial likelihood computation (for non-sequential models) across threads using `--cpus-per-task`  \n",
    "\n",
    "The first and third items I know how to implement. For the second, I need to learn how to make `ADDM.grid_search` and MPI job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.4",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "name": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
